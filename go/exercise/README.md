# 背景

​	在开始研究之前，想了一下用什么语言来实现。考虑到整个处理主要涉及到CSV文件等IO方面的处理，而且后续还得考虑第三方包，并发，测试等问题，感觉C++的基础组件之类的不太方便。为了能快速的试验一些想法，不会把时间浪费在一些语法，内存，组件的问题上，我采取了Go语言来完成。

​	经过一些考察，了解到数据库的联表操作（join）的基本实现方式大概有三种

- nested loop

- merge join

- hash join

  因为要求的数据量在百万级别（无序），表可以读进内存中，尽可能的提高速度，所以选择了hash join的方法，并尝试分析并实现。

  

# 实现

​	题外话，因为没有一个整体的概念，在开始实现之前，我用mysql进行了一次查询，想以此为基准开展后面的实现，可能是我的操作方式不对，在做完索引后的查询时间也需要几分钟，所以我放弃了这个想法......

​	针对hash join我做出了以下的解法

### solution1 （无启动goroutine）

​	起初最直接的想法，是直接的将两个csv文件,t1,t2全部读进内存，然后根据a列数据出现的次数构建起一个map，map的key值为a的值，value值为a在csv文件中出现的次数。所以读取两个csv文件后，在内存中拥有了两个文件对应的map。通过遍历map1，判断t1K是否存在于map2中，若存在,则计算t1V * t2V的值，累积到结果中。最后输出结果，完成算法。

​	在完成这个实现后，我开始思考改进的办法，一度陷入僵局。考虑到Go语言有goroutine后，我开始进行了一些想法的实验，花费了好几天的时间。

​	首先我想的是在io部分，将一个文件分割成多个部分，使用多个goroutine进行并行io读取。但是在测试之后发现并没有改进效果，因为主要的速度瓶颈在于io，所以多个goroutine没有提高读取的速度，反而还要在调度，同步协作等方面耗费等多的时间，所以第一次的优化思路在美好的期待下失败了。

### solution2  (使用goroutine 将读取与比较汇总的过程独立)

​	在优化solution1之后，我的整体优化思路也在往使用goroutine方面行动。在尝试多个goroutine之后，想到了一个办法，可以将使用csvReader将文件中数据解析为Record的过程作为一个goroutine启动，后续的map1与map2比较并汇总的过程作为另一个goroutine启动，两者通过chan来来传输数据。

​	我认为这样独立的两个过程，不用等待两个文件都读入内存后，再进行比较汇总，而且不用为第二个文件(t2)使用一个map表，节省了一定的内存。

### solution3(在solution2的基础上，使用多个goroutine进行比较)	

​	其实solution3的出现在solution2之前，但是排序按照难度循序渐进也就这样吧。在io方面优化无法前进时，我考虑的是在比较的时候，如果拆分第一个map1，然后启动多个goroutine尽量并行的与map2去进行比较并进行汇总，期望能达到一定速度上的提升。

​	但是在考察之后，找不到比较标准的拆分map的办法，所以我自己尝试的是，在csvReader将文件中数据解析为Record的过程中，按照后续计划分配的goroutine的个数，随机地向不同的chan进行传输（使用多个chan），传输给不同的goroutine进行比较汇总。同时为了不用锁减少损耗，我建立了一个count的slice，每个goroutine可以用自己的变量进行汇总，不用因为互斥变量去加锁，执行完后，再累积各个count的值，最后输出结果。

​	可能在数据分配的方面，或者多goroutine上的协作问题，我做的不好，这个优化的效果也不是很理想，在数据量小的情况下，测试时间基本上跑不过上面的解法。可能在数据量小的情况下goroutine无法跑满，而且得进行调度有所损耗。我尝试使用了千万级的数据，在一些时候测试速度会比solution2更快。

​	开始时候的想法，我是想到的是类似mapreduce的做法，将数据分解再汇总企图优化整体的速度，可能在实现的方面上有点问题。

### 优化题外话

​	在经过几次尝试后，好像在数据量较小的情况下优化的效果不太明显。在solution3上，我还尝试过使用多个goroutine来进行Record的解析并进行传输，但是读取的字节没有想到好的办法做一个分割，总会有解析错误，也耽误了不少时间。所以我尝试了在解析之后使用slice进行存放，然后再进行分割，想法是实现了，但是效果比较差，而且还增加了slice的内存，得不偿失，不过算是突发奇想，失败了。

​	在上述多种尝试无法改善后，我在只能重新转向io，发现了mmap，因为时间有限没有选择自己实现，采用的是第三方的mmap包。在采用了mmap的方式后，对比之前使用的buffio还是有些许提升，不过远没有文章上写的百倍那么夸张......

​	综上所述，尽自己所能地想办法优化，但是在数据量较小的情况下，可能还不如不优化来的好。或许是我的想法或者实现出现了偏差吧......



# 分析、思考

###性能分析方面

​	平时自己写代码，都是小打小闹，一般不会特别的去分析性能。这一次可以说是我第一次正经的做性能分析。因为刚好选择了go语言，自带的pprof工具可以简单的生成cpu,mem之类的调用信息。开始尝试的时候生成的svg十分复杂难以理解，后面改了一下代码，而且用go-torch生成了几个火焰图，并粗略地学习了火焰图的看法。

####solution1

​	因为solution1是最开始实现的，没有使用mmap，从cpu的火焰图中，调用栈一路向上，到最上面可以看出，对cpu的使用绝大部分来自于syscall，我认为是因为read进行的io操作进行了系统调用，想改善或许得从io函数下手或者使用mmap之类的。内存的消耗主要是来自来自FileToDictBuffIo函数，也就是将csv文件读取并构成hash表的过程，符合预期。我计算了一下，使用的是map[int]int，所以在64位机子下，假设csv文件中的a列500万个数各不相同(最多)，则有5000000\*8b*2，大概80m不到的空间，可以接受。

####solution2

​	solution2开始使用了goroutine和mmap，所以火焰图变得很复杂，可以看得出，主要的cpu调用依然在于读取的Read函数，但是底层不在是system调用，而是变成了memmove，应该是内存的映射。但是因为使用了额外的goroutine，开始有了runtime.usleep之类的调用，损耗不小。内存上内存中是缓存了一个hash表，我用go语言的benchmark简单测试了一下数据，内存占用量比solution1小。

####solution3

​	solution3在solution2的基础上采用了更多的goroutine在进行比较，结论与solution2类似，但是从图上看到

runtime.usleep的占比提高了不少，所以很大部分损耗在于goroutine的使用这一方面，所以我认为我在gorotine的分配和协作方面没有做好，在时间内没有优化得当。

####结论

​	从上面的分析看来，我所做的优化在这个数量级下效果并不太理想。虽然goroutine能够简易的事件并发，并行等工作，但并不是一个银弹，不能解决所有的问题，就如在小数据量的情况下runtime.usleep的损耗并不能忽略，反而会带来更多的损耗。

​	或许在直接使用mmap，并且只在内存中装载一个hash表，不使用额外的goroutine在这个数量级下才是最好的做法吧，简洁明了。



###number of distinct values 

​	distinct values越多，意味着在构建hash表的过程中，key的项越多，所以后续的比较阶段也会更大，所以内存和cpu的消耗会变大。

###做法

​	开头说到，数据库的联表操作（join）的基本实现方式大概有三种。nested loop，merge join，hash join。

- nested loop

  是以嵌套循环的方式，将两个表分别作为外部输入表和内部输入表，外部循环逐行消耗外部输入表。内部循环为每个外部行执行，在内部输入表中搜索匹配行。

  适用的场景外表记录比较小，而且内表索引性较好的情况下，保证开销较小。

- merge join

  应该确保两个关联表都是按照关联的字段进行排序，要求较为严格。merge join操作从每个表取一条记录开始匹配，如果符合关联条件，则放入结果集中；否则，将关联字段值较小的记录抛弃，从这条记录对应的表中取下一条记录继续进行匹配，直到整个循环结束。

  适用的场景与hash join差不多，如果行源已经被排过序，在执行排序合并连接时不需要再排序了，merge join的性能会优于hash join。

- hash join

  将表的关联字段作为hash key，构建hash表，然后在进行探测，根据计算出的hash值去hash表中寻找匹配的记录。

  适用场景适用于两个表的数据量差别很大的情况。

  但是由于hash join需要在内存中建立hash表，如果表的大小太大无法一次性的装进内存，就应该分成若干个partition，进行多次io，同时hash join在创建hash表和进行hash值计算时会对cpu进行消耗。但整体只需要扫描两张表一次，整体效率高。



​	在sql方面之前看过tidb的源码解析文章，应该还可以在优化器上对sql进行优化。在IO上只读取需要作计算的列的数据。将所需要的计算如count下推到分布式的集群少，计算后再汇总，减少rpc调用的次数减少消耗。或者类似maprudece，在数据足够大的情况下，缓解单机的压力，提高并行度，将数据有效的分配的多个机器上，做完相应的计算再汇总相应的节点等。

​	以上是我的一些想法，不代表正确。

